#!/usr/bin/env elixir

# Quick Start GiGPO Demo
# Run this script to see GiGPO in action!

Mix.install([
  {:jason, "~> 1.4"}
])

Code.require_file("lib/lmstudio.ex")
Code.require_file("lib/lmstudio/quantum_classical_hybrid.ex")
Code.require_file("lib/lmstudio/gigpo_trainer.ex")

defmodule QuickGiGPODemo do
  @moduledoc """
  Quick demonstration of GiGPO for LLM agent training.
  
  Just run: elixir run_gigpo_demo.exs
  """
  
  alias LMStudio.{GiGPOTrainer, QuantumClassicalHybrid}
  
  def run do
    IO.puts """
    
    ðŸš€ GiGPO Quick Demo
    ==================
    
    Group-in-Group Policy Optimization (GiGPO) for LLM Agent Training
    
    Key Features:
    âœ“ Hierarchical advantage estimation (episode + step level)
    âœ“ Anchor state grouping for fine-grained credit assignment  
    âœ“ Critic-free, memory-efficient training
    âœ“ Works with sparse rewards and long horizons
    
    """
    
    # Start required systems
    {:ok, _qc_pid} = QuantumClassicalHybrid.start_link()
    {:ok, _pid} = GiGPOTrainer.start_link([
      group_size: 4,
      discount_factor: 0.95,
      step_weight: 1.0
    ])
    
    # Quick demo scenarios
    demo_simple_task()
    demo_complex_task()
    show_results_summary()
    
    IO.puts "\nðŸŽ‰ GiGPO Demo Complete! Check out the full demos for more details."
  end
  
  def demo_simple_task do
    IO.puts """
    
    ðŸ  Demo 1: Simple Kitchen Task
    =============================
    
    Task: "Heat an egg and put it on countertop"
    
    This demonstrates GiGPO's ability to:
    â€¢ Group actions from the same kitchen state
    â€¢ Learn which appliances work better for heating
    â€¢ Assign fine-grained credit to individual actions
    """
    
    env_config = %{
      type: :kitchen,
      initial_state: %{location: "kitchen", step_count: 0},
      action_space: ["go to fridge", "take egg", "go to microwave", "heat egg", "put egg"],
      reward_function: :task_completion,
      max_steps: 20
    }
    
    {:ok, result} = GiGPOTrainer.train_episode_group(
      "heat an egg and put it on countertop",
      env_config
    )
    
    display_quick_results("Kitchen Task", result)
  end
  
  def demo_complex_task do
    IO.puts """
    
    ðŸ›’ Demo 2: Online Shopping Task  
    ==============================
    
    Task: "Find wireless headphones under $100"
    
    This demonstrates GiGPO's ability to:
    â€¢ Group actions from the same webpage state
    â€¢ Learn optimal click sequences
    â€¢ Handle sparse rewards (success only at purchase)
    """
    
    env_config = %{
      type: :shopping,
      initial_state: %{page: "search", step_count: 0},
      action_space: ["search[query]", "click[item]", "add to cart", "buy now"],
      reward_function: :purchase_success,
      max_steps: 15
    }
    
    {:ok, result} = GiGPOTrainer.train_episode_group(
      "find wireless headphones under $100",
      env_config
    )
    
    display_quick_results("Shopping Task", result)
  end
  
  def show_results_summary do
    IO.puts """
    
    ðŸ“Š What Just Happened?
    =====================
    
    GiGPO Training Process:
    
    1. ðŸ“‹ Rollout Phase
       â€¢ Generated 4 agent trajectories per task
       â€¢ Each trajectory tried different action sequences
       â€¢ Collected states, actions, and rewards
    
    2. ðŸŽ¯ Anchor State Grouping  
       â€¢ Identified repeated states across trajectories
       â€¢ Grouped actions taken from the same state
       â€¢ Created step-level groups for comparison
    
    3. ðŸ“Š Hierarchical Advantages
       â€¢ Episode advantages: "How good was this trajectory overall?"
       â€¢ Step advantages: "How good was this action in this context?"
       â€¢ Combined both for fine-grained credit assignment
    
    4. ðŸ”„ Policy Update
       â€¢ Updated agent policy using combined advantages
       â€¢ Improved both overall strategy and specific actions
       â€¢ Maintained computational efficiency
    
    Key Benefits Demonstrated:
    âœ“ Better credit assignment than episode-level only methods
    âœ“ Learns from repeated situations across trajectories  
    âœ“ No extra memory or computational overhead
    âœ“ Suitable for long-horizon, sparse-reward tasks
    
    Paper Results:
    â€¢ 13.3% improvement on ALFWorld tasks
    â€¢ 10.6% improvement on WebShop tasks
    â€¢ Same efficiency as GRPO baseline
    """
  end
  
  defp display_quick_results(task_name, result) do
    if result.step_metrics.total_anchor_states > 0 do
      IO.puts """
      
      âœ“ #{task_name} Results:
        â€¢ Success Rate: #{Float.round(result.episode_metrics.success_rate * 100, 1)}%
        â€¢ Anchor States Found: #{result.step_metrics.total_anchor_states}
        â€¢ Actions Grouped: #{result.step_metrics.total_grouped_steps}
        â€¢ Training Time: #{result.timing.total_time}ms
      
      ðŸŽ¯ Anchor state grouping enabled fine-grained learning!
      """
    else
      IO.puts """
      
      âœ“ #{task_name} Results:
        â€¢ Success Rate: #{Float.round(result.episode_metrics.success_rate * 100, 1)}%
        â€¢ No repeated states (fell back to episode-level only)
        â€¢ Training Time: #{result.timing.total_time}ms
      """
    end
  end
end

# Run the quick demo
QuickGiGPODemo.run()